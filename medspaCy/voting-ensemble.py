import argparse
import glob
import os
import re

from tqdm import tqdm

import spacy
import medspacy
import cassis

## TODO - refactor type system creation to be importable from a separate file

def main( args , classifiers ):
    ############################
    ## Create a type system
    ## - https://github.com/dkpro/dkpro-cassis/blob/master/cassis/typesystem.py
    with open( os.path.join( args.typesDir , 'Sentence.xml' ) , 'rb' ) as fp:
        typesystem = cassis.load_typesystem( fp )
    SentenceAnnotation = typesystem.get_type( 'org.apache.ctakes.typesystem.type.textspan.Sentence' )
    ############
    ## ... for tokens
    TokenAnnotation = typesystem.create_type( name = 'uima.tt.TokenAnnotation' , 
                                              supertypeName = 'uima.tcas.Annotation' )
    typesystem.add_feature( type_ = TokenAnnotation ,
                            name = 'text' , 
                            rangeTypeName = 'uima.cas.String' )
    ############
    ## ... for Metadata
    ## TODO - this parent and supertype should probably be something else
    NoteMetadata = typesystem.create_type( name = 'refsem.Metadata' ,
                                           supertypeName = 'uima.tcas.Annotation' )
    ## TODO - how to represent pairs, as per the reference standard?
    typesystem.add_feature( type_ = NoteMetadata ,
                            name = 'other' ,
                            description = '' ,
                            rangeTypeName = 'uima.cas.String' )
    ############
    ## ... for UmlsConcept
    UmlsConcept = typesystem.create_type( name = 'refsem.UmlsConcept' ,
                                          supertypeName = 'uima.tcas.Annotation' )
    typesystem.add_feature( type_ = UmlsConcept ,
                            name = 'tui' ,
                            description = '' ,
                            rangeTypeName = 'uima.cas.String' )
    typesystem.add_feature( type_ = UmlsConcept ,
                            name = 'cui' ,
                            description = '' ,
                            rangeTypeName = 'uima.cas.String' )
    ############
    ## ... for IdentifiedAnnotation
    IdentifiedAnnotation = typesystem.create_type( name = 'textsem.IdentifiedAnnotation' ,
                                                   supertypeName = 'uima.tcas.Annotation' )
    typesystem.add_feature( type_ = IdentifiedAnnotation ,
                            name = 'ontologyConceptArray' ,
                            description = 'The xmi:id of the array of ontology concepts associated with this annotation' ,
                            rangeTypeName = 'uima.cas.Integer' )
    typesystem.add_feature( type_ = IdentifiedAnnotation ,
                            name = 'discoveryTechnique' ,
                            description = 'The index of the discovery technique (or classifier) that produced this annotation' ,
                            rangeTypeName = 'uima.cas.Integer' )
    typesystem.add_feature( type_ = IdentifiedAnnotation ,
                            name = 'confidence' ,
                            description = 'A confidence score [0-1] generated by the discoveryTechnique when generating producing this annotation' ,
                            rangeTypeName = 'uima.cas.Double' )
    ############
    ## ... for OMOP CDM v5.3 NOTE_NLP table properties
    NoteNlp = typesystem.create_type( name = 'edu.musc.tbic.omop_cdm.Note_Nlp_TableProperties' ,
                                      supertypeName = 'uima.tcas.Annotation' )
    typesystem.add_feature( type_ = NoteNlp ,
                            name = 'note_id' ,
                            description = '' ,
                            rangeTypeName = 'uima.cas.Integer' )
    typesystem.add_feature( type_ = NoteNlp ,
                            name = 'section_concept_id' ,
                            description = '' ,
                            rangeTypeName = 'uima.cas.Integer' )
    typesystem.add_feature( type_ = NoteNlp ,
                            name = 'snippet' ,
                            description = '' ,
                            rangeTypeName = 'uima.cas.String' )
    typesystem.add_feature( type_ = NoteNlp ,
                            name = 'offset' ,
                            description = '' ,
                            rangeTypeName = 'uima.cas.Integer' )
    typesystem.add_feature( type_ = NoteNlp ,
                            name = 'lexical_variant' ,
                            description = '' ,
                            rangeTypeName = 'uima.cas.String' )
    typesystem.add_feature( type_ = NoteNlp ,
                            name = 'note_nlp_concept_id' ,
                            description = '' ,
                            rangeTypeName = 'uima.cas.Integer' )
    ## TODO - this really should be an int but we can't look up the appropriate
    ##        ID without a connected OMOP CDM Concept table
    typesystem.add_feature( type_ = NoteNlp ,
                            name = 'note_nlp_source_concept_id' ,
                            description = '' ,
                            rangeTypeName = 'uima.cas.String' )
    typesystem.add_feature( type_ = NoteNlp ,
                            name = 'nlp_system' ,
                            description = '' ,
                            rangeTypeName = 'uima.cas.String' )
    typesystem.add_feature( type_ = NoteNlp ,
                            name = 'term_exists' ,
                            description = '' ,
                            rangeTypeName = 'uima.cas.String' )
    typesystem.add_feature( type_ = NoteNlp ,
                            name = 'term_temporal' ,
                            description = '' ,
                            rangeTypeName = 'uima.cas.String' )
    typesystem.add_feature( type_ = NoteNlp ,
                            name = 'term_modifiers' ,
                            description = '' ,
                            rangeTypeName = 'uima.cas.String' )
    ############################
    ## Iterate over the files, covert to CAS, and write the XMI to disk
    filenames = []
    if( args.fileList is None ):
        filenames = [ os.path.basename( f ) for f in glob.glob( os.path.join( args.inputDir ,
                                                                              '*.xmi' ) ) ]
    else:
        with open( args.fileList , 'r' ) as fp:
            for line in fp:
                line = line.strip()
                filenames.append( line )
    ##
    for filename in tqdm( filenames ,
                          desc = 'Voting (k={})'.format( len( classifiers ) ) ):
        ##
        classifier2id = {}
        id2classifier = {}
        xmiId2cui = {}
        ##
        if( args.fileList is None ):
            xmi_filename = filename
        else:
            xmi_filename = '{}.xmi'.format( filename )
        with open( os.path.join( args.inputDir , xmi_filename ) , 'rb' ) as fp:
            cas = cassis.load_cas_from_xmi( fp ,
                                            typesystem = typesystem )
        ## Grab the discoveryTechnique long names
        for metadata in cas.select( 'refsem.Metadata' ):
            classifier_name , classifier_id = metadata.other.split( '=' )
            if( classifier_id == '0' or
                classifier_id not in classifiers ):
                continue
            classifier2id[ classifier_name ] = classifier_id
            id2classifier[ classifier_id ] = classifier_name
        ## Grab all the CUIs and their xmi:id's
        for umls_concept in cas.select( 'refsem.UmlsConcept' ):
            xmi_id = umls_concept.xmiID
            cui = umls_concept.cui
            xmiId2cui[ xmi_id ] = cui
        ## For sentence-based annotations, we want to seed the kb with
        ## all the spans of the sentences
        if( args.votingUnit == 'sentence' ):
            kb = {}
            for sentence in cas.select( 'org.apache.ctakes.typesystem.type.textspan.Sentence' ):
                begin_offset = sentence.begin
                end_offset = sentence.end
                span = '{}-{}'.format( begin_offset , end_offset )
                kb[ span ] = {}
                kb[ span ][ 'begin_offset' ] = begin_offset
                kb[ span ][ 'end_offset' ] = end_offset
                kb[ span ][ 'norm_counts' ] = {}
                kb[ span ][ 'norm_weights' ] = {}
                ## TODO - update when changing how CUI works for sections/sentences
                kb[ span ][ 'norm_counts' ][ 'section' ] = 0
                kb[ span ][ 'norm_weights' ][ 'section' ] = 0
        else:
            kb = {}
        ## Grab all...
        for annot in cas.select( 'textsem.IdentifiedAnnotation' ):
            technique = annot.discoveryTechnique
            ## TODO - refactor variable named 'cui' to reflect
            ##        more fluid use across annotation types
            if( args.votingUnit == 'sentence' ):
                cui = 'section'
            elif( args.votingUnit == 'span' ):
                concept_id = int( annot.ontologyConceptArray )
                cui = xmiId2cui[ concept_id ]
            else:
                ## TODO - adapt to context attributes
                cui = 1
            begin_offset = annot.begin
            end_offset = annot.end
            span = '{}-{}'.format( begin_offset , end_offset )
            if( technique == '0' or
                technique not in classifiers ):
                continue
            if( args.votingUnit == 'span' ):
                if( span not in kb ):
                    kb[ span ] = {}
                    kb[ span ][ 'begin_offset' ] = begin_offset
                    kb[ span ][ 'end_offset' ] = end_offset
                    kb[ span ][ 'norm_counts' ] = {}
                    kb[ span ][ 'norm_weights' ] = {}
                if( cui not in kb[ span ][ 'norm_counts' ] ):
                    kb[ span ][ 'norm_counts' ][ cui ] = 0
                    kb[ span ][ 'norm_weights' ][ cui ] = 0
                kb[ span ][ 'norm_counts' ][ cui ] += 1
                kb[ span ][ 'norm_weights' ][ cui ] += 1
            elif( args.votingUnit == 'sentence' ):
                if( span in kb ):
                    weight = 1
                    kb[ span ][ 'norm_counts' ][ cui ] += 1
                    kb[ span ][ 'norm_weights' ][ cui ] += weight
                else:
                    for ref_span in kb:
                        if( end_offset < kb[ ref_span ][ 'begin_offset' ] ):
                            ## span ends before this sentence
                            continue
                        if( begin_offset > kb[ ref_span ][ 'end_offset' ] ):
                            ## span starts after this sentence
                            continue
                        if( begin_offset >= kb[ ref_span ][ 'begin_offset' ] and
                            end_offset <= kb[ ref_span ][ 'end_offset' ] ):
                            ## span is inside this sentence
                            weight = 0.5
                            kb[ ref_span ][ 'norm_counts' ][ cui ] += 1
                            kb[ ref_span ][ 'norm_weights' ][ cui ] += weight
                            break
                        if( begin_offset <= kb[ ref_span ][ 'begin_offset' ] and
                            end_offset >= kb[ ref_span ][ 'end_offset' ] ):
                            ## span fully contains this sentence
                            weight = 1
                            kb[ ref_span ][ 'norm_counts' ][ cui ] += 1
                            kb[ ref_span ][ 'norm_weights' ][ cui ] += weight
                        elif( begin_offset >= kb[ ref_span ][ 'begin_offset' ] and
                            begin_offset < kb[ ref_span ][ 'end_offset' ] ):
                            ## span starts inside this sentence
                            weight = 0.5
                            kb[ ref_span ][ 'norm_counts' ][ cui ] += 1
                            kb[ ref_span ][ 'norm_weights' ][ cui ] += weight
                        elif( end_offset > kb[ ref_span ][ 'begin_offset' ] and
                              end_offset <= kb[ ref_span ][ 'end_offset' ] ):
                            ## span ends inside this sentence
                            weight = 0.5
                            kb[ ref_span ][ 'norm_counts' ][ cui ] += 1
                            kb[ ref_span ][ 'norm_weights' ][ cui ] += weight
        ##
        for span in kb:
            max_cui = 'CUI-less'
            max_cui_count = 0
            ## TODO - allow option to choose CUI based on weighted score
            if( args.votingUnit == 'sentence' ):
                max_norm_weights = kb[ span ][ 'norm_weights' ][ 'section' ]
            for cui in kb[ span ][ 'norm_counts' ]:
                cui_count = kb[ span ][ 'norm_counts' ][ cui ]
                if( cui_count > max_cui_count and
                    cui_count >= args.minVotes ):
                    max_cui = cui
                    max_cui_count = cui_count
                    if( args.votingUnit == 'span' ):
                        max_norm_weights = kb[ span ][ 'norm_weights' ][ cui ]
            if( args.zeroStrategy == 'drop' and
                ( ( args.votingUnit == 'span' and max_cui_count == 0 ) or
                  ( args.votingUnit == 'sentence' and max_norm_weights < args.minVotes ) ) ):
                continue
            ## TODO - add special flag to set whether we want to track CUIs or not
            modifiers = [ 'confidence={}'.format( max_cui_count / len( classifiers ) ) ,
                          'weighted_confidence={}'.format( max_norm_weights / len( classifiers ) ) ]
            if( args.votingUnit == 'sentence' ):
                note_nlp = NoteNlp( begin = kb[ span ][ 'begin_offset' ] ,
                                    end = kb[ span ][ 'end_offset' ] ,
                                    offset = kb[ span ][ 'begin_offset' ] ,
                                    nlp_system = 'Majority Voting Ensemble System' ,
                                    term_modifiers = ';'.join( modifiers ) )
            elif( args.votingUnit == 'span' ):
                note_nlp = NoteNlp( begin = kb[ span ][ 'begin_offset' ] ,
                                    end = kb[ span ][ 'end_offset' ] ,
                                    offset = kb[ span ][ 'begin_offset' ] ,
                                    nlp_system = 'Majority Voting Ensemble System' ,
                                    note_nlp_source_concept_id = max_cui ,
                                    term_modifiers = ';'.join( modifiers ) )
            cas.add_annotation( note_nlp )
        if( args.outputDir is not None ):
            cas.to_xmi( path = os.path.join( args.outputDir , xmi_filename ) ,
                        pretty_print = True )


if __name__ == '__main__':
    parser = argparse.ArgumentParser( description = 'Simple voting ensemble system' )
    parser.add_argument( '--types-dir' ,
                         dest = 'typesDir' ,
                         help = 'Directory containing the systems files need to be loaded' )
    parser.add_argument( '--input-dir' ,
                         required = True ,
                         dest = 'inputDir' ,
                         help = 'Input directory containing CAS XMI files' )
    parser.add_argument( '--file-list' , default = None ,
                         dest = 'fileList' ,
                         help = 'File containing the basename of all files in order' )
    parser.add_argument( '--ref-dir' ,
                         default = None ,
                         dest = 'refDir' ,
                         help = 'Output directory for writing the reference CAS XMI files to' )
    parser.add_argument( '--classifier-list' ,
                         default = '1234567890' ,
                         dest = 'classifierList' ,
                         help = 'List of classifiers (by id) to include' )
    parser.add_argument( '--min-votes' ,
                         default = 1 ,
                         dest = 'minVotes' ,
                         help = 'Minimum numbers of votes required to win a (majority) vote' )
    parser.add_argument( '--zero-strategy' ,
                         default = 'CUI-less' ,
                         choices = [ 'drop' , 'CUI-less' ] ,
                         dest = 'zeroStrategy' ,
                         help = 'When no normalizations receives enough votes, use this strategy to select a normalization' )
    parser.add_argument( '--voting-unit' ,
                         default = 'span' ,
                         choices = [ 'span' , 'sentence' , 'doc' ] ,
                         dest = 'votingUnit' ,
                         help = 'The unit that accumulates votes: "span" means to treat annotations as their own unit, "sentence" means to converge on all spans matching a sentence, "doc" aggregates all annotations at the document level' )
    parser.add_argument( '--tie-breaker' ,
                         default = 'random' ,
                         choices = [ 'random' , 'ranked' , 'confidenceScore' ] ,
                         dest = 'tieBreaker' ,
                         help = 'When multiple normalizations receive the same number of votes, use this strategy to resolve ties' )
    parser.add_argument( '--rank-file' ,
                         default = None ,
                         dest = 'rankFile' ,
                         help = 'File containing the ranked list of classifiers to be used in tie-breakers (earlier classifiers are chosen over later classifiers' )
    parser.add_argument( '--output-dir' ,
                         default = None ,
                         dest = 'outputDir' ,
                         help = 'Output directory for writing the oracle consolidated CAS XMI files to' )
    args = parser.parse_args()
    classifiers = []
    for i in range( 0 , len( args.classifierList ) ):
        if( args.classifierList[ i ] == '0' ):
            classifiers.append( '10' )
        else:
            classifiers.append( args.classifierList[ i ] )
    args.minVotes = int( args.minVotes )
    main( args , classifiers )
